---
title: "Problem Set #1"
subtitle: "BST 258: Causal Inference -- Theory and Practice"
author: "Salvador Balkus"
date: ""
execute: 
  cache: true
format:
  pdf:
    documentclass: scrartcl
    papersize: letter
    fontsize: 11pt
    geometry:
      - margin=1in
      - heightrounded
    number-sections: false
    colorlinks: true
    link-citations: true
    callout-appearance: simple
    callout-icon: false
    # figure options
    fig-width: 6
    fig-asp: 0.618
    fig-cap-location: bottom
    # code block options
    code-line-numbers: false
    code-block-bg: false
    highlight-style: gruvbox
bibliography: refs.bib
---

```{julia}
#| echo: false
#| output: false
using Pkg
cd(@__DIR__)
Pkg.activate(".")
```

## Question 1

The GitHub Repository for this assignment can be found at the following link: 

[https://github.com/salbalkus/bst258_ps1](https://github.com/salbalkus/bst258_ps1)

Note that I am using Julia, so dependencies are tracked using the Project.toml and Manifest.toml files.

## Question 2

Consider a completely randomized experiment (CRE) with $m$ units treated.

### (a)

What is the marginal distribution of the treatment indicator $A$?

:::{.callout-note title="Answer"}
Assuming $A_i \in \{0, 1\}$, the marginal distribution of the first sampled unit $i$ is $P(A_i = 1) = \frac{m}{n}$ and $P(A_i = 0) = 1 - \frac{m}{n} = \frac{n - m}{n}$.
:::

### (b)

What is the joint distribution of $A_i$ and $A_j$
for two units $i \neq j$?

:::{.callout-note title="Answer"}
WLOG, assume that $A_i$'s treatment is sampled first, then $A_j$. Then, 

$$P(A_i = a_i, A_j = a_j) = P(A_j = a_j | A_i = a_i) \cdot P(A_i = a_i)$$

Plugging in part (a) and noting that the number of total units (and possibly treated units) for $A_j$ is reduced by 1 after sampling $A_i$, we find that the joint distribution of $A_i$ and $A_j$ is given by:

- $P(A_i = 1, A_j = 1) = \frac{m}{n} \cdot \frac{m-1}{n-1} = \frac{m(m-1)}{n(n-1)}$
- $P(A_i = 0, A_j = 1) = \frac{m}{n} \cdot \Big(1 - \frac{m-1}{n-1}\Big) = \frac{m(n-m)}{n(n-1)}$
- $P(A_i = 1, A_j = 0) = \Big(1 - \frac{m}{n}\Big) \cdot \Big(\frac{m}{n-1}\Big) = \frac{m(n-m)}{n(n-1)}$
- $P(A_i = 0, A_j = 0) = \Big(1 - \frac{m}{n}\Big) \cdot \Big(1 - \frac{m-1}{n-1}\Big) = \frac{(n-m)^2}{n(n-1)}$

Then the joint pmf is given by the following table:

| | | |
|:---------:|:------------------------:|:------------------------:|
|           |  $A_i = 0$               | $A_i = 1$                |
| $A_j = 0$ | $\frac{(n-m)^2}{n(n-1)}$ | $\frac{m(n-m)}{n(n-1)}$  |
| $A_j = 1$ | $\frac{m(n-m)}{n(n-1)}$  | $\frac{m(m-1)}{n(n-1)}$  |
|-----------|--------------------------|--------------------------|


:::

### (c)

What are $\mathbb{V}(A_i)$ and $Cov(A_i, A_j)$ for $i \neq j$?

:::{.callout-note title="Answer"}

**Variance**: Since $A_i$ is marginally $\text{Bernoulli}(\frac{m}{n})$, the variance is known to be 

$$\mathbb{V}(A_i) = \frac{m}{n}(1 - \frac{m}{n}) = \frac{m}{n}(\frac{n - m}{n})$$

Note that $E(A_i) = P(A_i = 1) = \frac{m}{n}$; then, one can derive this as 

$$\mathbb{V}(A_i) = E(A_i^2) - E(A_i)^2 = \sum_{a_i = 0}^1 a_i^2\cdot P(A_i = a_i) - (\frac{m}{n})^2 = \frac{m}{n}(1 - \frac{m}{n})$$

which gives the same result.

**Covariance**: Using the property of covariance that $Cov(A_i, A_j) = E(A_iA_j) - E(A_i)E(A_j)$, we can first note that $E(A_i) = E(A_j) = P(A_i = 1) = \frac{m}{n}$. Additionally, $E(A_iA_j) = P(A_i = 1, A_j = 1) = \frac{m(m-1)}{n(n-1)}$ from the joint probability table solved in part (b). Therefore, the covariance is

$$Cov(A_i, A_j) = \frac{m(m-1)}{n(n-1)} - \frac{m^2}{n^2} = \frac{nm^2 - nm - m^2n + m^2}{n^2(n-1)} = -\frac{1}{n}(\frac{m(n-m)}{n(n-1)})$$

:::

### (d)

What is the sample ATT in expectation?

:::{.callout-note title="Answer"}

We can compute the expectation of the sample ATT using the linearity of expectation, noting that $A_i \perp\!\!\!\!\perp Y_i(a)$ by definition of a CRE counterfactual outcome, and that $E(A_i) = P(A_i = 1) = \frac{m}{n}$. Therefore, we have

\begin{align*}
E(\theta^{ATT}) &= \frac{1}{m}\sum_{i=1}^nE(A_i(Y_i(1) - Y_i(0)))\\
&= \frac{n}{m} E(A_i) E(Y_i(1) - Y_i(0))\\
&= \frac{n}{m} \frac{m}{n} E(Y_i(1) - Y_i(0))\\
&= E(Y_i(1) - Y_i(0)) = \theta^{ATE}
\end{align*}

So, in expectation, the sample ATT is equal to the population ATE (that is, $E(\theta^{ATT}) = \theta^{ATE}$).

:::

## Question 3

Let $Y_i(1) = Y_i(0) + \theta$. Show that $\mathbb{V}(Y_i(1)) = \mathbb{V}(Y_i(0))$ and the correlation equals 1, where expectations are sample expectations.

:::{.callout-note title="Answer"}

Since $Y_i(1) = Y_i(0) + \theta$, we have the property of variance that $\mathbb{V}(X + b) = \mathbb{V}(X)$. Even when considering the sample variance, this follows because

$$\mathbb{V}(X + b) = \mathbb{E}((X + b - \mathbb{E}(X) - b)^2) = \mathbb{E}((X - \mathbb{E}(X))^2) = \mathbb{V}(X)$$

with $b$ canceling due to linearity of expectation. Therefore, substituting in the given equality we have 

$$\mathbb{V}(Y_i(1)) = \mathbb{V}(Y_i(0) + \theta) = \mathbb{V}(Y_i(0))$$

Using this, we can note the similar property of covariance that $Cov(X + b, Y + c) = Cov(X, Y)$; in the sample setting this follows from

$$Cov(X + b, Y + c) = \mathbb{E}((X + b - \mathbb{E}(X + b))(X + c - \mathbb{E}(X + c))) = \mathbb{E}((X - \mathbb{E}(X))(X - \mathbb{E}(X)))= Cov(X, Y)$$

Using this property, we have that


\begin{align*}
\rho(Y_i(1), Y_i(0)) &= \frac{Cov(Y_i(1), Y_i(0))}{\sqrt{\mathbb{V}(Y_i(1))\cdot\mathbb{V}(Y_i(0))}}\\
&= \frac{Cov(Y_i(0) + \theta, Y_i(0))}{\sqrt{\mathbb{V}(Y_i(0))\cdot \mathbb{V}(Y_i(0))}}\\
&= \frac{Cov(Y_i(0), Y_i(0))}{\sqrt{\mathbb{V}(Y_i(0))^2}}= \frac{\mathbb{V}(Y_i(0))}{\mathbb{V}(Y_i(0))} = 1
\end{align*}


thereby completing the proof. $\square$

:::


## Question 4

It‚Äôs tea time: (a hologram of) R.A. Fisher places eight cups of tea (with milk) in front of you and
asks you to identify which cups had tea poured before milk and vice-versa. Prior to giving you
the cups of tea, Fisher poured milk before tea in four of them and tea before milk in the other
four. The ordering in which the cups have been served is random. What is the probability that
you correctly guess 0, 1, 2, 3, or 4 of all of the cups that had tea poured first?

:::{.callout-note title="Answer"}
There are 70 possible ways to choose 4 out of 8 cups as those that had tea poured first (thereby leaving the remaining 4 as those that had milk poured first). This is because ${8 \choose 4} = \frac{8!}{4!(8-4)!} = 70$. Therefore, the probability of guessing $k$ cups correctly is ${4 \choose k} \cdot {4 \choose 4-k} \cdot \frac{1}{70}$. That is, the probability is the product the number of possible ways to choose $k$ cups correctly and the number of possible ways to choose the remaining $4-k$ cups incorrectly, divided by the total number of possible ways to choose 4 cups.

Hence, we have:

- Probability of 0 cups correct: $\frac{1}{70}$ (only one possibility, all four wrong)
- Probability of 1 cup correct: ${4\choose 3} \cdot {4\choose 1} \cdot \frac{1}{70} = \frac{16}{70}$
- Probability of 2 cups correct: ${4\choose 2} \cdot {4\choose 2} \cdot \frac{1}{70} = \frac{36}{70}$
- Probability of 3 cups correct: ${4\choose 1} \cdot {4\choose 3} \cdot \frac{1}{70} = \frac{16}{70}$
- Probability of 4 cups correct: $\frac{1}{70}$ (only one possibility, all four correct)

:::


## Question 5

### (a)

Describe possible factors that might have contributed to this seemingly contradictory result.

:::{.callout-note title="Answer"}
*First*, we can notice that those with large kidney stones received Treatment A much more often (263/343 times, ~77%), while those with small kidney stones received Treatment B more often (234/357 times, ~66%). *Second*, we can also note that those who experienced large stones had a lower probability of being successfully treated across both treatments (73% large versus 93% small for A, 69% large versus 87% small for B).

Putting these two facts together, we can conclude that Treatment B probably appeared more successful in aggregate because it was more frequently used to treat small kidney stones, the type of outcome more likely to be treated succesfully regardless of treatment type. However, in reality, if we "control" for outcome type via stratification, Treatment A is actually more effective -- which is probably why it was more frequently used for cases of large kidney stones that were "more difficult" to treat.
:::

### (b)

Alarmed by this discrepancy, your colleague asks you to further segment the results by reported gender. This newly refined look at the data suggests that for both small and large
kidney stones, treatment ùêµ is consistently more effective than treatment ùê¥ across all genders. Construct a hypothetical (i.e., candidate) table that illustrates this.

:::{.callout-note title="Answer"}

| | | |
|:-------------:|:-------------:|:-------------:|
|               | Treatment A   | Treatment B   |
| Male, Small   | 95% (74/78)   | 99% (69/70)   |
| Female, Small | 77% (7/9)     | 83% (165/200) |
| Male, Large   | 49% (21/43)   | 62% (37/60)   |
| Female, Large | 78% (171/220) | 90% (18/20)   |
| All           | 78% (273/350) | 83% (289/350) |
|---------------|---------------|---------------|

:::

### (c)

What is this phenomenon (it has a name)? What are its broader implications and significance
in the interpretation of data?

:::{.callout-note title="Answer"}
This phenomenon is known as **Simpson's Paradox** (@Pearl2014): when the association between two variables reverses upon conditioning of a third variable. For causal inference, it implies that in order to obtain a true causal relationship, we need to control for confounding variables that might change the probability of a given treatment assignment. Otherwise, a situation might arise in which one treatment appears better only because it was more likely to be assigned to an individual more likely to exhibit a positive outcome -- not because it is more effective.
:::


## Question 6

 Conduct a simulation study to illustrate the difference between Fisher's sharp null and Neyman's weak null.

:::{.callout-note title="Answer"}

First, we can load the necessary packages and define a function to run the simulation.

```{julia}
using Distributions
using Plots
using Random

function run_trial(Œº‚ÇÄ, Œº‚ÇÅ, œÉ¬≤, n, Œ±, n_sim, B)

  # sample counterfactuals
  Y1 = rand(Normal(Œº‚ÇÅ, sqrt(œÉ¬≤)), n)
  Y0 = rand(Normal(Œº‚ÇÄ, sqrt(œÉ¬≤)), n)

  # sample treatment assignment matrix
  A = stack(broadcast(n -> sample(vcat(ones(n√∑2), zeros(n√∑2)), n, 
    replace = false), fill(n, n_sim)))
  Y = map(A -> (@. A * Y1 + (1 - A) * Y0), eachcol(A))

  # calculate difference-in-means test statistic across simulations
  m1(Y, A) = mean(Y[A .== 1]) 
  m0(Y, A) = mean(Y[A .== 0])
  dif_in_means(Y, A) = abs(m1(Y, A) - m0(Y, A))
  œà_ate = [dif_in_means(Y[i], A[:, i]) for i in 1:n_sim]

  # calculate sharp null p-values
  sharp_null(Y, A, œà) = mean(dif_in_means(Y, shuffle(A)) >= œà for _ in 1:B)
  p_sharp = [sharp_null(Y[i], A[:, i], œà_ate[i]) for i in 1:n_sim]

  # calculate weak null p-values
  p_est = mean(A, dims = 1)
  var_ate(Y, A, p) = (var(Y[A .== 1]) / p) + (var(Y[A .== 0]) / (1 - p))
  œÉ¬≤_est = [var_ate(Y[i], A[:, i], p_est[i]) for i in 1:n_sim]
  p_weak = @. 2*(1 - cdf(Normal(0, sqrt(œÉ¬≤_est / n)), œà_ate))

  # calculate power of the trial
  power_sharp = mean(p_sharp .< Œ±)
  power_weak = mean(p_weak .< Œ±)

  return power_sharp, power_weak
end


```


Then, we can actually run the simulation with the specified parameters:
```{julia}
Random.seed!(1)
ns = [20, 50, 100, 200, 500]
Œº‚ÇÄ = 0
Œº‚ÇÅ = 1/10
œÉ¬≤ = 1/16
Œ± = 0.05
n_sim = 1000
B = 10000

result = [run_trial(Œº‚ÇÄ, Œº‚ÇÅ, œÉ¬≤, n, Œ±, n_sim, B) for n in ns]
power_sharp = [r[1] for r in result]
power_weak = [r[2] for r in result]

println("Sharp Power: " * string(power_sharp))
println("Weak Power: " * string(power_weak))

```

Finally, we plot the output of the experiment:
```{julia}
plot([power_sharp, power_weak], 
          marker = :circle,
          xticks = (1:5, string.(ns)),
          xaxis = "Samples", 
          yaxis = "Power",
          labels = ["Sharp Null" "Weak Null"])

```

**Comments**: The power of the weak null hypothesis test is (slightly) higher than the power of the sharp null hypothesis test across all sample sizes, though they converge to the same value at $n = 500$. This is consistent with the fact that Fisher's sharp null implies Neyman's weak null, but that the converse is not necessarily true; the power of the weak null detects all cases where the sharp null is true, and some where the sharp null is not true. However, given that the difference is quite small, it is likely that the sharp null and weak null tests will have similar power in practice, at least for reasonably large sample sizes.

:::

{{< pagebreak >}}

## Question 7

You have been tasked with estimating the average treatment effect, for which it suffices to solve the least squares program.

### (a)

Solve the linear program in $(\alpha, \beta)$.

:::{.callout-note title="Answer"}

We can solve the linear program

$\min_{\alpha,\beta} \frac{1}{2n} \sum_{i=1}^n(Y_i - \alpha - \beta A_i)^2$

using the first derivative test, setting the partial derivatives to 0 to obtain solutions:


\begin{align*}
  \frac{\partial}{\partial \alpha} L(\alpha, \beta) &= -\frac{1}{n} \sum_{i=1}^n(Y_i - \alpha - \beta A_i) = 0\\
  \implies \hat{\alpha} &=  \beta \bar{A} - \bar{Y}
\end{align*}


and


\begin{align*}
  \frac{\partial}{\partial \beta} L(\alpha, \beta) &= -\frac{1}{n} \sum_{i=1}^n(Y_i - \alpha - \beta A_i)A_i = 0\\
  \implies \hat{\beta} &= \frac{\alpha \bar{A} - \frac{1}{n}\sum_{i=1}^nA_iY_i}{\bar{A^2}}
\end{align*}



Plugging in the solution for $\alpha$ into the solution for $\beta$, we obtain


\begin{align*}
\beta &= \frac{\beta(\bar{A})^2 - \bar{A}\bar{Y} - \frac{1}{n}\sum_{i=1}^nA_iY_i}{\bar{A^2}}\\
\implies \beta(\frac{\bar{A}^2 - \bar{A^2}}{\bar{A^2}}) &= \frac{\bar{A}\bar{Y} - \frac{1}{n}\sum_{i=1}^nA_iY_i}{\bar{A^2}}\\
\implies \hat{\beta} &= \frac{\sum(Y_i - \bar{Y})(A_i - \bar{A}}{\sum(A_i - \bar{A})^2}
\end{align*}


and therefore

$$\hat{\alpha} =  \hat{\beta}\bar{A} - \bar{Y} = \hat{\beta} = \frac{\sum(Y_i - \bar{Y})(A_i - \bar{A})}{\sum(A_i - \bar{A})^2}\bar{A} - \bar{Y}$$

By the second derivative test, we can take the partial derivatives to get


\begin{align*}
\frac{\partial}{\partial \alpha^2} L(\alpha, \beta) &= n\\
\frac{\partial}{\partial \beta^2} L(\alpha, \beta) &= \frac{1}{n}\sum_{i=1}^nA_i^2\\
\frac{\partial}{\partial \alpha\partial \beta} L(\alpha, \beta) &= \bar{A}
\end{align*}


Now, since $A \in \{0, 1\}$, we have 



\begin{align*}
D(\alpha, \beta) &= n\cdot \frac{1}{n}\sum_{i=1}^nA_i^2 - \bar{A}^2 = \\
\sum_{i=1}^nA_i - (\frac{1}{n}\sum_{i=1}^nA_i)^2 > 0
\end{align*}


and by the second derivative test, $D(\alpha, \beta) > 0$ and $\frac{\partial}{\partial \alpha^2} L(\alpha, \beta) = n > 0$ implies $(\hat{\alpha}, \hat{\beta})$ is a minimum. It must be a global minimum since it was found to be a unique critical point in the first derivative test, therefore the endpoints cannot be smaller.


Therefore, one solution to the linear program is the simple linear regression equation,

$$\hat{\beta} = \frac{\sum(Y_i - \bar{Y})(A_i - \bar{A}}{\sum(A_i - \bar{A})^2}$$

and

$$\hat{\alpha} = \frac{\sum(Y_i - \bar{Y})(A_i - \bar{A}}{\sum(A_i - \bar{A})^2}\bar{A} - \bar{Y}$$

However, we can simplify $\hat{\beta}$ even further. Note that by construction of the completely randomized experiment, $\bar{A} = \frac{m}{n}$. Therefore,


\begin{align*}
\sum_{i=1}^n (A_i - \bar{A})^2 &= sum_{i=1}^nA_i^2 - 2\frac{m}{n}sum_{i=1}^nA-i + sum_{i=1}^n(\frac{m}{n})^2\\
&= m - 2m + \frac{m^2}{n} = \frac{m(m-n)}{n}
\end{align*}


Furthermore, since the numerator of $\hat{\beta}$ is $\sum_{i=1}^n(Y_i - \bar{Y})(A_i - \bar{A})$, we can simplify as follows using the same aforementioned properties of $A_i$. We start by writing in terms of a step we already solved previously, and then rewrite algebraically in terms of $\bar{Y}_1$ and $\bar{Y}_0$:

\begin{align*}
\sum_{i=1}^n(Y_i - \bar{Y})(A_i - \bar{A}) &= m\bar{Y} - \frac{m}{n}\bar{Y}_1\\
&= \frac{1}{n}(\sum A_iY_i + \sum (1 - A_i)Y_i)) - \frac{1}{m}\sum A_iY_i\\
&= \frac{m}{n}((m-n)\bar{Y}_1 + (n-m)\bar{Y}_0)\\
&= \frac{m(m-n)}{n}(\bar{Y}_1 - \bar{Y}_0)
\end{align*}

Therefore, plugging this into the numerator, $\hat{\beta} = \frac{m(m-n)}{n}(\bar{Y}_1 - \bar{Y}_0) / \frac{m(m-n)}{n} = \bar{Y}_1 - \bar{Y}_0$, which is the difference-in-means estimator! It follows that $\hat{\alpha} = (\bar{Y}_1 - \bar{Y}_0)\bar{A} - \bar{Y}$.

:::

### (b)

Is $\hat{\beta}$ a valid estimator of the ATE?

:::{.callout-note title="Answer"}

Yes, $\hat{\beta}$ is a valid estimator of the ATE. We can show this by proving it is unbiased; that $E(\hat{\beta}) = \beta)$. Recall from (a) that $\sum_{i=1}^n (A_i - \bar{A})^2 = \frac{m(m-n)}{n}$. Then, noting that the numerator of $\hat{\beta}$ is $n(\bar{Y})(\bar{A}) - \frac{1}{n}\sum_{i=1}^n A_i Y_i$ from part (a), we can substitute $Y_i = A_i Y_i(1) + (1 - A_i) Y_i(0)$ and use linearity of expectation to evaluate the left and right terms of the subtraction separately.

The expectation of the first term in the subtraction reduces to


\begin{align*}
E(\bar{A}\bar{Y}) &= \frac{m}{n}E(A_iY_i(1) + (1-A_i)Y_i(0))\\
&= \frac{m^2}{n} E(Y_i(1)) + \frac{m(n-m)}{n}E(Y_i(0))
\end{align*}



The expectation of the second term in the subtraction reduces to 


\begin{align*}
E(\frac{1}{n}\sum_{i=1}^n A_i Y_i) &= E(A_i^2Y_i(1) + A_i(1 - A_i)Y_i(1))\\
&= \frac{m}{n}E(Y_i(1))
\end{align*}


Now, combining these terms and dividing by the denominator $\frac{m(m-n)}{n}$ solved previously, we have



\begin{align*}
E(\hat{\beta}) &= (m(\frac{m-n}{n})E(Y_i(1)) - \frac{m(m-n)}{n}E(Y_i(0)) -  \frac{m}{n}E(Y_i(1))) / \frac{m(m-n)}{n}\\
&= (m(\frac{m-n}{n})E(Y_i(1)) - \frac{m(m-n)}{n}E(Y_i(0))) / \frac{m(m-n)}{n} \\
&= E(Y_i(0)) - E(Y_i(1))
\end{align*}


which is the definition of the ATE. Therefore, $E(\hat{\beta}) = E(Y_i(0) - Y_i(1))$ meaning it is an unbiased, and therefore valid estimator of the ATE.

:::

# Acknowledgements
A disclosure for academic honesty: GitHub Copilot was used to prepare this assignment, however, only very minor uses of the text and code autocomplete feature were used. 

## References

::: {#refs}
:::



