{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Problem Set #1\"\n",
        "subtitle: \"BST 258: Causal Inference -- Theory and Practice\"\n",
        "author: \"Salvador Balkus\"\n",
        "date: \"\"\n",
        "format:\n",
        "  pdf:\n",
        "    documentclass: scrartcl\n",
        "    papersize: letter\n",
        "    fontsize: 11pt\n",
        "    geometry:\n",
        "      - margin=1in\n",
        "      - heightrounded\n",
        "    number-sections: false\n",
        "    colorlinks: true\n",
        "    link-citations: true\n",
        "    callout-appearance: simple\n",
        "    callout-icon: false\n",
        "    # figure options\n",
        "    fig-width: 6\n",
        "    fig-asp: 0.618\n",
        "    fig-cap-location: bottom\n",
        "    # code block options\n",
        "    code-line-numbers: false\n",
        "    code-block-bg: false\n",
        "    highlight-style: gruvbox\n",
        "bibliography: refs.bib\n",
        "---"
      ],
      "id": "4f7fbe0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "using Pkg\n",
        "cd(@__DIR__)\n",
        "Pkg.activate(\".\")"
      ],
      "id": "cfb6fac7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2\n",
        "\n",
        "### (a)\n",
        "\n",
        ":::{.callout-note title=\"Answer\"}\n",
        "Assuming $A_i \\in \\{0, 1\\}$, the marginal distribution of the first sampled unit $i$ is $P(A_i = 1) = \\frac{m}{n}$ and $P(A_i = 0) = 1 - \\frac{m}{n}$.\n",
        ":::\n",
        "\n",
        "### (b)\n",
        "\n",
        ":::{.callout-note title=\"Answer\"}\n",
        "WLOG, assume that $A_i$'s treatment is sampled first, then $A_j$. Then, \n",
        "\n",
        "$P(A_i = a_i, A_j = a_j) = P(A_j = a_j | A_i = a_i) \\cdot P(A_i = a_i)$\n",
        "\n",
        "Plugging in part (a) and noting that the number of total units and treated units for $A_j$ is reduced by 1 after sampling $A_i$, we find that the joint distribution of $A_i$ and $A_j$ is given by:\n",
        "\n",
        "- $P(A_i = 1, A_j = 1) = \\frac{m}{n} \\cdot \\frac{m-1}{n-1} = \\frac{m(m-1)}{n(n-1)}$\n",
        "- $P(A_i = 0, A_j = 1) = \\frac{m}{n} \\cdot \\Big(1 - \\frac{m-1}{n-1}\\Big) = \\frac{m(n-m)}{n(n-1)}$\n",
        "- $P(A_i = 1, A_j = 0) = \\Big(1 - \\frac{m}{n}\\Big) \\cdot \\Big(\\frac{m}{n-1}\\Big) = \\frac{m(n-m)}{n(n-1)}$\n",
        "- $P(A_i = 0, A_j = 0) = \\Big(1 - \\frac{m}{n}\\Big) \\cdot \\Big(1 - \\frac{m-1}{n-1}\\Big) = \\frac{(n-m)^2}{n(n-1)}$\n",
        "\n",
        "Then the joint pmf is given by the following table:\n",
        "\n",
        "|           |  $A_i = 0$              | $A_i = 1$ |\n",
        "| $A_j = 0$ | $\\frac{m(m-1)}{n(n-1)}$ | $\\frac{m(n-m)}{n(n-1)}$  |\n",
        "| $A_j = 1$ | $\\frac{m(n-m)}{n(n-1)}$ | $\\frac{(n-m)^2}{n(n-1)}$ |\n",
        "\n",
        ":::\n",
        "\n",
        "### (c)\n",
        "\n",
        "### (d)\n",
        "\n",
        "\n",
        "{{< pagebreak >}}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Question 3\n",
        "\n",
        ":::{.callout-note title=\"Answer\"}\n",
        "\n",
        "Since $Y_i(1) = Y_i(0) + \\theta$, we have the property of variance that $\\mathbb{V}(X + b) = \\mathbb{V}(X)$. Even when considering the sample variance, this follows because\n",
        "\n",
        "$\\mathbb{V}(X + b) = \\mathbb{E}((X + b - \\mathbb{E}(X + b))^2) = \\mathbb{E}((X - \\mathbb{E}(X))^2) = \\mathbb{V}(X)$\n",
        "\n",
        "with $b$ canceling due to linearity of expectation. Therefore, substituting in the given equality we have \n",
        "\n",
        "$\\mathbb{V}(Y_i(1)) = \\mathbb{V}(Y_i(0) + \\theta) = \\mathbb{V}(Y_i(0))$\n",
        "\n",
        "Using this, we can note the similar property of covariance that $Cov(X + b, Y + c) = Cov(X, Y)$; in the sample setting this follows from\n",
        "\n",
        "$Cov(X + b, Y + c) = \\mathbb{E}((X + b - \\mathbb{E}(X + b))(X + c - \\mathbb{E}(X + c))) = \\mathbb{E}((X - \\mathbb{E}(X))(X - \\mathbb{E}(X)))= Cov(X, Y)$\n",
        "\n",
        "Using this property, we have that\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\rho(Y_i(1), Y_i(0)) = \\frac{Cov(Y_i(1), Y_i(0))}{\\sqrt{\\mathbb{V}(Y_i(1))\\cdot\\mathbb{V}(Y_i(0))}}\\\\\n",
        "= \\frac{Cov(Y_i(0) + \\theta, Y_i(0))}{\\sqrt{\\mathbb{V}(Y_i(0))\\cdot \\mathbb{V}(Y_i(0))}}\\\\\n",
        "= \\frac{Cov(Y_i(0), Y_i(0))}{\\sqrt{\\mathbb{V}(Y_i(0))^2}}= \\frac{\\mathbb{V}(Y_i(0))}{\\mathbb{V}(Y_i(0))} = 1\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "thereby completing the proof. $\\square$\n",
        "\n",
        "::\n",
        "\n",
        "\n",
        "{{< pagebreak >}}\n",
        "\n",
        "\n",
        "\n",
        "## Question 4\n",
        "\n",
        ":::{.callout-note title=\"Answer\"}\n",
        "There are 70 possible ways to choose 4 out of 8 cups as those that had tea poured first (thereby leaving the remaining 4 as those that had milk poured first). This is because ${8 \\choose 4} = \\frac{8!}{4!(8-4)!} = 70$. \n",
        "\n",
        "Therefore, the probability of guessing $k$ cups correctly is ${4 \\choose k} \\cdot {4 \\choose 4-k} \\cdot \\frac{1}{70}$. That is, the probability is the product the number of possible ways to choose $k$ cups correctly and the number of possible ways to choose the remaining $4-k$ cups incorrectly, divided by the total number of possible ways to choose 4 cups.\n",
        "\n",
        "Hence, we have:\n",
        "\n",
        "- Probability of 0 cups correct: $\\frac{1}{70}$ (only one possibility, all four wrong)\n",
        "- Probability of 1 cup correct: ${4\\choose 3} \\cdot {4\\choose 1} \\cdot \\frac{1}{70} = \\frac{16}{70}$\n",
        "- Probability of 2 cups correct: ${4\\choose 2} \\cdot {4\\choose 2} \\cdot \\frac{1}{70} = \\frac{36}{70}$\n",
        "- Probability of 3 cups correct: ${4\\choose 1} \\cdot {4\\choose 3} \\cdot \\frac{1}{70} = \\frac{16}{70}$\n",
        "- Probability of 4 cups correct: $\\frac{1}{70}$ (only one possibility, all four correct)\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "{{< pagebreak >}}\n",
        "\n",
        "\n",
        "\n",
        "## Question 5\n",
        "\n",
        "### (a)\n",
        "\n",
        ":::{.callout-note title=\"Answer\"}\n",
        "*First*, we can notice that those with large kidney stones received Treatment A much more often (263/343 times, ~77%), while those with small kidney stones received Treatment B more often (234/357 times, ~66%). *Second*, we can also note that those who experienced large stones had a lower probability of being successfully treated across both treatments (73% large versus 93% small for A, 69% large versus 87% small for B).\n",
        "\n",
        "Putting these two facts together, we can conclude that Treatment B probably appeared more successful in aggregate because it was more frequently used to treat small kidney stones, the type of outcome more likely to be treated succesfully regardless of treatment type. However, in reality, if we \"control\" for outcome type via stratification, Treatment A is actually more effective -- which is probably why it was more frequently used for cases of large kidney stones that were \"more difficult\" to treat.\n",
        ":::\n",
        "\n",
        "### (b)\n",
        "\n",
        ":::{.callout-note title=\"Answer\"}\n",
        "\n",
        "|               | Treatment A   | Treatment B   |\n",
        "| Male, Small   | 95% (74/78)   | 99% (69/70)   |\n",
        "| Female, Small | 77% (7/9)     | 83% (165/200) |\n",
        "| Male, Large   | 49% (21/43)   | 62% (37/60)   |\n",
        "| Female, Large | 78% (171/220) | 90% (18/20)   |\n",
        "| All           | 78% (273/350) | 83% (289/350) |\n",
        "\n",
        ":::\n",
        "\n",
        "### (c)\n",
        "\n",
        ":::{.callout-note title=\"Answer\"}\n",
        "This phenomenon is known as **Simpson's Paradox** (@Pearl2014): when the association between two varaibles reverses upon conditioning of a third variable. For causal inference, it implies that in order to obtain a true causal relationship, we need to control for confounding variables that might change the probability of a given treatment assignment. Otherwise, a situation might arise in which one treatment appears better only because it was more likely to be assigned to an individual more likely to exhibit a positive outcome -- not because it is more effective.\n",
        ":::\n",
        "\n",
        "\n",
        "{{< pagebreak >}}\n",
        "\n",
        "\n",
        "\n",
        "## Question 6\n"
      ],
      "id": "26938c3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "using Distributions\n",
        "using Plots\n",
        "using Random\n",
        "\n",
        "function run_trial(μ₀, μ₁, σ², n, α, n_sim, B)\n",
        "\n",
        "  # sample counterfactuals\n",
        "  Y1 = rand(Normal(μ₁, sqrt(σ²)), n)\n",
        "  Y0 = rand(Normal(μ₀, sqrt(σ²)), n)\n",
        "\n",
        "  # sample treatment assignment matrix\n",
        "  A = rand(Bernoulli(0.5), n, n_sim)\n",
        "  Y = map(A -> (@. A * Y1 + (1 - A) * Y0), eachcol(A))\n",
        "\n",
        "  # calculate difference-in-means test statistic across simulations\n",
        "  m1(Y, A) = mean(Y[A .== 1]) \n",
        "  m0(Y, A) = mean(Y[A .== 0])\n",
        "  dif_in_means(Y, A) = abs(m1(Y, A) - m0(Y, A))\n",
        "  ψ_ate = [dif_in_means(Y[i], A[:, i]) for i in 1:n_sim]\n",
        "\n",
        "  # calculate sharp null p-values\n",
        "  sharp_null(Y, A, ψ) = mean(dif_in_means(Y, shuffle(A)) >= ψ for _ in 1:B)\n",
        "  p_sharp = [sharp_null(Y[i], A[:, i], ψ_ate[i]) for i in 1:n_sim]\n",
        "\n",
        "  # calculate weak null p-values\n",
        "  p_est = mean(A, dims = 1)\n",
        "  var_ate(Y, A, p) = (var(Y[A .== 1]) / p) + (var(Y[A .== 0]) / (1 - p))\n",
        "  σ²_est = [var_ate(Y[i], A[:, i], p_est[i]) for i in 1:n_sim]\n",
        "  p_weak = @. 2*(1 - cdf(Normal(0, sqrt(σ²_est / n)), ψ_ate))\n",
        "\n",
        "  # calculate power of the trial\n",
        "  power_sharp = mean(p_sharp .< α)\n",
        "  power_weak = mean(p_weak .< α)\n",
        "\n",
        "  return power_sharp, power_weak\n",
        "end\n"
      ],
      "id": "0847dd3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Random.seed!(1)\n",
        "ns = [20, 50, 100, 200, 500]\n",
        "μ₀ = 0\n",
        "μ₁ = 1/10\n",
        "σ² = 1/16\n",
        "α = 0.05\n",
        "n_sim = 1000\n",
        "B = 10000\n",
        "\n",
        "result = [run_trial(μ₀, μ₁, σ², n, α, n_sim, B) for n in ns]\n",
        "power_sharp = [r[1] for r in result]\n",
        "power_weak = [r[2] for r in result]"
      ],
      "id": "1b0658d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot([power_sharp, power_weak], \n",
        "          marker = :circle,\n",
        "          xticks = (1:5, string.(ns)),\n",
        "          xaxis = \"Samples\", \n",
        "          yaxis = \"Power\",\n",
        "          labels = [\"Sharp Null\" \"Weak Null\"])\n",
        "\n",
        "savefig()"
      ],
      "id": "82480267",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{{< pagebreak >}}\n",
        "\n",
        "\n",
        "\n",
        "## Question 7\n",
        "\n",
        "### (a)\n",
        "\n",
        "We can solve the linear program\n",
        "\n",
        "$\\min_{\\alpha,\\beta} \\frac{1}{2n} \\sum_{i=1}^n(Y_i - \\alpha - \\beta A_i)^2$\n",
        "\n",
        "using the first derivative test, setting the partial derivatives to 0 to obtain solutions:\n",
        "\n",
        "\n",
        "```{math}\n",
        "\\begin{align*}\n",
        "  \\frac{\\partial}{\\partial \\alpha} L(\\alpha, \\beta) = -\\frac{1}{n} \\sum_{i=1}^n(Y_i - \\alpha - \\beta A_i) = 0\\\\\n",
        "  \\implies \\hat{\\alpha} =  \\beta \\bar{A} - \\bar{Y}\n",
        "\\end{align*}\n",
        "```\n",
        "\n",
        "\n",
        "and\n",
        "\n",
        "\n",
        "```{math}\n",
        "\\begin{align*}\n",
        "  \\frac{\\partial}{\\partial \\beta} L(\\alpha, \\beta) = -\\frac{1}{n} \\sum_{i=1}^n(Y_i - \\alpha - \\beta A_i)A_i = 0\\\\\n",
        "  \\implies \\hat{\\beta} = \\frac{\\alpha \\bar{A} - \\frac{1}{n}\\sum_{i=1}^nA_iY_i}{\\bar{A^2}}\n",
        "\\end{align*}\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Plugging in the solution for $\\alpha$ into the solution for $\\beta$, we obtain\n",
        "\n",
        "\n",
        "```{math}\n",
        "\\begin{align*}\n",
        "\\beta = \\frac{\\beta(\\bar{A})^2 - \\bar{A}\\bar{Y} - \\frac{1}{n}\\sum_{i=1}^nA_iY_i}{\\bar{A^2}}\\\\\n",
        "\\implies \\beta(\\frac{\\bar{A}^2 - \\bar{A^2}}{\\cancel{\\bar{A^2}}}) = \\frac{\\bar{A}\\bar{Y} - \\frac{1}{n}\\sum_{i=1}^nA_iY_i}{\\cancel{\\bar{A^2}}}\\\\\n",
        "\\implies \\hat{\\beta} = \\frac{\\sum(Y_i - \\bar{Y})(A_i - \\bar{A}}{\\sum(A_i - \\bar{A})^2}\n",
        "\\end{align*}\n",
        "```\n",
        "\n",
        "\n",
        "and therefore\n",
        "\n",
        "\n",
        "```{math}\n",
        "\\hat{\\alpha} =  \\hat{\\beta}\\bar{A} - \\bar{Y} = \\hat{\\beta} = \\frac{\\sum(Y_i - \\bar{Y})(A_i - \\bar{A}}{\\sum(A_i - \\bar{A})^2}\\bar{A} - \\bar{Y}\n",
        "```\n",
        "\n",
        "\n",
        "By the second derivative test, we can take the partial derivatives to get\n",
        "\n",
        "\n",
        "```{math}\n",
        "\\begin{align*}\n",
        "\\frac{\\partial}{\\partial \\alpha^2} L(\\alpha, \\beta) = n\\\\\n",
        "\\frac{\\partial}{\\partial \\beta^2} L(\\alpha, \\beta) = \\frac{1}{n}\\sum_{i=1}^nA_i^2\\\\\n",
        "\\frac{\\partial}{\\partial \\alpha\\partial \\beta} L(\\alpha, \\beta) = \\bar{A}\n",
        "\\end{align*}\n",
        "```\n",
        "\n",
        "\n",
        "Now, since $A \\in \\{0, 1\\}$, we have \n",
        "\n",
        "\n",
        "\n",
        "```{math}\n",
        "\\begin{align*}\n",
        "D(\\alpha, \\beta) = n\\cdot \\frac{1}{n}\\sum_{i=1}^nA_i^2 - \\bar{A} = \\\\\n",
        "\\sum_{i=1}^nA_i - \\frac{1}{n}\\sum_{i=1}^nA_i = \\frac{n-1}{n} > 0\n",
        "\\end{align*}\n",
        "```\n",
        "\n",
        "\n",
        "and by the second derivative test, $D(\\alpha, \\beta) > 0$ and $\\frac{\\partial}{\\partial \\alpha^2} L(\\alpha, \\beta) = n > 0$ implies $(\\hat{\\alpha}, \\hat{\\beta})$ is a minimum. It must be a global minimum since it was found to be a unique critical point in the first derivative test, therefore the endpoints cannot be smaller.\n",
        "\n",
        "Therefore, the solution to the linear program is\n",
        "\n",
        "$\\hat{\\beta} = \\frac{\\sum(Y_i - \\bar{Y})(A_i - \\bar{A}}{\\sum(A_i - \\bar{A})^2}$\n",
        "\n",
        "and\n",
        "\n",
        "$\\hat{\\alpha} = \\frac{\\sum(Y_i - \\bar{Y})(A_i - \\bar{A}}{\\sum(A_i - \\bar{A})^2}\\bar{A} - \\bar{Y}$\n",
        "\n",
        "### (b)\n",
        "\n",
        "Yes, $\\hat{\\beta} is a valid estimator of the ATE. We can show this by proving it is unbiased; that $E(\\hat{\\beta}) = \\beta)$. Note that by construction of the completely randomized experiment, $\\bar{A} = \\frac{m}{n}$. Therefore,\n",
        "\n",
        "\n",
        "\n",
        "```{math}\n",
        "\\begin{align*}\n",
        "\\sum_{i=1}^n (A_i - \\bar{A})^2 = sum_{i=1}^nA_i^2 - 2\\frac{m}{n}sum_{i=1}^nA-i + sum_{i=1}^n(\\frac{m}{n})^2\\\\\n",
        "= m - 2m + \\frac{m^2}{n} = \\frac{m(m-n)}{n}\n",
        "\\end{align*}\n",
        "```\n",
        "\n",
        "\n",
        "Therefore, it is a constant that can be moved outside the expectation.\n",
        "\n",
        "Then, noting that  the numerator of $\\hat{\\beta}$ is $(\\hat{Y})(\\hat{A}) - \\frac{1}{n}\\sum_{i=1}^n A_i Y_i$ from part (a), we can substitute $Y_i = A_i Y_i(1) + (1 - A_i) Y_i(0)$ and use linearity of expectation to evaluate the left and right terms of the subtraction separately.\n",
        "\n",
        "The expectation of the first term in the subtraction reduces to\n",
        "\n",
        "\n",
        "```{math}\n",
        "\\begin{align*}\n",
        "E(\\bar{A}\\bar{Y}) = \\frac{m}{n}E(A_iY_i(1) + (1-A_i)Y_i(0))\\\\\n",
        "= \\frac{m^2}{n} E(Y_i(1)) + \\frac{m(n-m)}{n}E(Y_i(0))\n",
        "\\end{align*}\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "The expectation of the second term in the subtraction reduces to \n",
        "\n",
        "\n",
        "```{math}\n",
        "\\begin{align*}\n",
        "E(\\frac{1}{n}\\sum_{i=1}^n A_i Y_i) = E(A_i^2Y_i(1) + A_i(1 - A_i)Y_i(1)) = \\frac{m}{n}E(Y_i(1))\n",
        "\\end{align*}\n",
        "```\n",
        "\n",
        "\n",
        "Now, combining these terms and dividing by the denominator $\\frac{m(m-n)}{n}$ solved previously, we have\n",
        "\n",
        "\n",
        "\n",
        "```{math}\n",
        "\\begin{align*}\n",
        "E(\\hat{\\beta}) = (m(\\frac{m-n}{n})E(Y_i(1)) - \\frac{m(m-n)}{n}E(Y_i(0)) -  \\frac{m}{n}E(Y_i(1))) / \\frac{m(m-n)}{n}\\\\\n",
        "= (m(\\frac{m-n}{n})E(Y_i(1)) - \\frac{m(m-n)}{n}E(Y_i(0))) / \\frac{m(m-n)}{n} \\\\\n",
        "= E(Y_i(0)) - E(Y_i(1))\n",
        "```\n",
        "\n",
        "\n",
        "which is the definition of the ATE. Therefore, $E(\\hat{\\beta}) = E(Y_i(0) - Y_i(1))$ meaning it is an unbiased, and therefore valid estimator of the ATE.\n",
        "\n",
        "# Acknowledgements\n",
        "A disclosure for academic honesty: GitHub Copilot was used to prepare this assignment. However, only minor uses of the text/code autocomplete feature were used. \n",
        "\n",
        "## References\n",
        "\n",
        "::: {#refs}\n",
        ":::\n"
      ],
      "id": "72f1a7de"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.10",
      "language": "julia",
      "display_name": "Julia 1.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}